{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7920e46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-0OH3S55:4042\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>data_model</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x282b5ed25b0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ[\"PYSPARK_PYTHON\"]=\"C:\\\\Spark\\\\spark-3.3.2-bin-hadoop2\\\\python\"\n",
    "#os.environ[\"PYSPARK_PYTHON\"]=\"C:\\\\Users\\\\snksh\\\\OneDrive\\\\Desktop\\\\Research\\\\sw\\\\python\\\\python\"\n",
    "os.environ[\"JAVA_HOME\"] = \"C:\\Program Files\\Java\\jdk-18.0.2.1\"\n",
    "os.environ[\"SPARK_HOME\"] = \"C:\\Spark\\spark-3.3.2-bin-hadoop2\"\n",
    "os.environ[\"PYLIB\"] = os.environ[\"SPARK_HOME\"] + \"/python/lib\"\n",
    "sys.path.insert(0, os.environ[\"PYLIB\"] + \"/py4j-0.10.9.5-src.zip\")\n",
    "sys.path.insert(0, os.environ[\"PYLIB\"] + \"/pyspark.zip\")\n",
    "\n",
    "MAX_MEMORY = \"14G\"\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"data_model\") \\\n",
    "    .config(\"spark.driver.memory\", MAX_MEMORY) \\\n",
    "    .getOrCreate()    \n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d18f139",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "302b00b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age_imputed: double (nullable = true)\n",
      " |-- bg2_lactate_imputed: double (nullable = true)\n",
      " |-- basophils_imputed: double (nullable = true)\n",
      " |-- eosinophils_imputed: double (nullable = true)\n",
      " |-- lymphocytes_imputed: double (nullable = true)\n",
      " |-- monocytes_imputed: double (nullable = true)\n",
      " |-- neutrophils_imputed: double (nullable = true)\n",
      " |-- albumin_imputed: double (nullable = true)\n",
      " |-- aniongap_imputed: double (nullable = true)\n",
      " |-- bicarbonate_imputed: double (nullable = true)\n",
      " |-- bun_imputed: double (nullable = true)\n",
      " |-- calcium_imputed: double (nullable = true)\n",
      " |-- chloride_imputed: double (nullable = true)\n",
      " |-- creatinine_imputed: double (nullable = true)\n",
      " |-- glucose_imputed: double (nullable = true)\n",
      " |-- sodium_imputed: double (nullable = true)\n",
      " |-- potassium_imputed: double (nullable = true)\n",
      " |-- inr_imputed: double (nullable = true)\n",
      " |-- pt_imputed: double (nullable = true)\n",
      " |-- ptt_imputed: double (nullable = true)\n",
      " |-- hematocrit_imputed: double (nullable = true)\n",
      " |-- hemoglobin_imputed: double (nullable = true)\n",
      " |-- mch_imputed: double (nullable = true)\n",
      " |-- mchc_imputed: double (nullable = true)\n",
      " |-- mcv_imputed: double (nullable = true)\n",
      " |-- platelet_imputed: double (nullable = true)\n",
      " |-- rbc_imputed: double (nullable = true)\n",
      " |-- rdw_imputed: double (nullable = true)\n",
      " |-- wbc_imputed: double (nullable = true)\n",
      " |-- scr_min_imputed: double (nullable = true)\n",
      " |-- ckd_imputed: double (nullable = true)\n",
      " |-- mdrd_est_imputed: double (nullable = true)\n",
      " |-- scr_baseline_imputed: double (nullable = true)\n",
      " |-- alt_imputed: double (nullable = true)\n",
      " |-- alp_imputed: double (nullable = true)\n",
      " |-- ast_imputed: double (nullable = true)\n",
      " |-- bilirubin_total_imputed: double (nullable = true)\n",
      " |-- charlson_comorbidity_index_imputed: double (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "63887"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_data = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").csv(\"../data/EDA/final_data_for_modelling.csv\")\n",
    "cluster_data = cluster_data.drop(\"_c0\")\n",
    "cluster_data.printSchema()\n",
    "cluster_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b90679",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# convert all values to double\n",
    "#import pyspark.sql.functions as F\n",
    "#numeric_cols = model_data.columns\n",
    "#numeric_cols.remove(\"gender\")\n",
    "#model_data = model_data.select(*(F.round(F.col(c).cast(\"double\"), 2).alias(c) for c in numeric_cols), \"gender\")\n",
    "#print(model_data.printSchema())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4276c153",
   "metadata": {},
   "source": [
    "#### Data model building"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a00d92",
   "metadata": {},
   "source": [
    "#### data pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3dff4cc",
   "metadata": {},
   "source": [
    "- datatype conversion\n",
    "- outlier handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa6736cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['age_imputed', 'bg2_lactate_imputed', 'basophils_imputed', 'eosinophils_imputed', 'lymphocytes_imputed', 'monocytes_imputed', 'neutrophils_imputed', 'albumin_imputed', 'aniongap_imputed', 'bicarbonate_imputed', 'bun_imputed', 'calcium_imputed', 'chloride_imputed', 'creatinine_imputed', 'glucose_imputed', 'sodium_imputed', 'potassium_imputed', 'inr_imputed', 'pt_imputed', 'ptt_imputed', 'hematocrit_imputed', 'hemoglobin_imputed', 'mch_imputed', 'mchc_imputed', 'mcv_imputed', 'platelet_imputed', 'rbc_imputed', 'rdw_imputed', 'wbc_imputed', 'scr_min_imputed', 'ckd_imputed', 'mdrd_est_imputed', 'scr_baseline_imputed', 'alt_imputed', 'alp_imputed', 'ast_imputed', 'bilirubin_total_imputed', 'gender_imputed']\n",
      "['gender']\n"
     ]
    }
   ],
   "source": [
    "feature_cols =  cluster_data.columns\n",
    "feature_cols.remove(\"charlson_comorbidity_index_imputed\")\n",
    "feature_cols.remove(\"gender\")\n",
    "feature_cols.append(\"gender_imputed\")\n",
    "print(feature_cols)\n",
    "\n",
    "categorical_cols = [\"gender\"]\n",
    "print(categorical_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2af2403",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature transformation for categorical features\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "si = StringIndexer(inputCols=categorical_cols, outputCols=[c+'_idx' for c in categorical_cols], handleInvalid=\"keep\")\n",
    "ohe = OneHotEncoder(inputCols=[c+'_idx' for c in categorical_cols], outputCols=[c+'_imputed' for c in categorical_cols], handleInvalid=\"keep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d367478",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "assembler = VectorAssembler( inputCols = feature_cols, outputCol = 'features')\n",
    "scaler = StandardScaler( inputCol = 'features', outputCol = 'standardized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53c94b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "pipeline =  Pipeline(stages = [si, ohe, assembler, scaler])\n",
    "data_model = pipeline.fit(cluster_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce8cbfcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data model\n",
    "data_model.write().overwrite().save(\"../model/data_model.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615d4b0e",
   "metadata": {},
   "source": [
    "#### Prediction model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d509c1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f83033",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data into train and test (Remember you are expected to compare the model later)\n",
    "train, test = model_data.randomSplit([0.7,0.3])\n",
    "print(\"Size of training data {0}\".format(train.count()))\n",
    "print(\"Size of test data {0}\".format(test.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8c00e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model train\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "lr = LinearRegression( featuresCol=\"standardized\", labelCol=\"charlson_comorbidity_index_imputed\")\n",
    "\n",
    "model_input = data_model.transform(train)\n",
    "model = lr.fit(model_input)\n",
    "\n",
    "# save model\n",
    "model.write().overwrite().save(\"gender_predict_model.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1880e67",
   "metadata": {},
   "source": [
    "#### model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1578bb83",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictionoutput = model.transform(model_input)\n",
    "predictionoutput.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1eb907d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictionoutput.select(\"charlson_comorbidity_index\", \"prediction\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8689374c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate with test data\n",
    "\n",
    "model_input_test = data_model.transform(test)\n",
    "\n",
    "pred_results = model.evaluate(model_input_test)\n",
    "pred_results.predictions.select(\"charlson_comorbidity_index\", \"prediction\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56afdda9",
   "metadata": {},
   "source": [
    "#### model from hyperparameter tuning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beae8659",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use a ParamGridBuilder to construct a grid of parameters to search over.\n",
    "# TrainValidationSplit will try all combinations of values and determine best model using\n",
    "# the evaluator.\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit\n",
    "from  pyspark.ml.evaluation import RegressionEvaluator\n",
    "lr2 = LinearRegression( featuresCol=\"standardized\", labelCol=\"charlson_comorbidity_index_imputed\")\n",
    "paramGrid = ParamGridBuilder()\\\n",
    "    .baseOn({lr2.labelCol: 'charlson_comorbidity_index_imputed'}) \\\n",
    "    .baseOn([lr2.predictionCol, 'prediction']) \\\n",
    "    .addGrid(lr2.regParam, [0.1, 0.01]) \\\n",
    "    .addGrid(lr2.fitIntercept, [False, True])\\\n",
    "    .addGrid(lr2.elasticNetParam, [0.0, 0.5, 1.0])\\\n",
    "    .build()\n",
    "\n",
    "\n",
    "\n",
    "# In this case the estimator is simply the linear regression.\n",
    "# A TrainValidationSplit requires an Estimator, a set of Estimator ParamMaps, and an Evaluator.\n",
    "eva = RegressionEvaluator()\n",
    "eva.setLabelCol(\"charlson_comorbidity_index_imputed\")\n",
    "tvs = TrainValidationSplit(estimator=lr2,\n",
    "                           estimatorParamMaps=paramGrid,\n",
    "                           evaluator=eva,\n",
    "                           # 80% of the data will be used for training, 20% for validation.\n",
    "                           trainRatio=0.8)\n",
    "\n",
    "# Run TrainValidationSplit, and choose the best set of parameters.\n",
    "model_input = data_model.transform(train)\n",
    "model = tvs.fit(model_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6a0755",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictionoutput = model.transform(model_input)\n",
    "predictionoutput.select(\"charlson_comorbidity_index_imputed\", \"prediction\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80e26ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_input_test = data_model.transform(test)\n",
    "\n",
    "pred_results = model.transform(model_input_test)\n",
    "pred_results.select(\"charlson_comorbidity_index_imputed\", \"prediction\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f34703b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.validationMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f7925a",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = [{p.name: v for p, v in m.items()} for m in model.getEstimatorParamMaps()]\n",
    "\n",
    "pd.DataFrame.from_dict([\n",
    "    {model.getEvaluator().getMetricName(): metric, **ps} \n",
    "    for ps, metric in zip(params, model.validationMetrics)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb67c60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.bestModel.extractParamMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c465dd01",
   "metadata": {},
   "outputs": [],
   "source": [
    "bm = model.bestModel\n",
    "model_input_test = data_model.transform(test)\n",
    "\n",
    "pred_results = bm.evaluate(model_input_test)\n",
    "print(pred_results.meanSquaredError)\n",
    "print(pred_results.r2)\n",
    "print(pred_results.rootMeanSquaredError)\n",
    "print(pred_results.r2adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891827c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "bm = model.bestModel\n",
    "model_input_test = data_model.transform(test)\n",
    "\n",
    "pred_results = bm.evaluate(model_input_test)\n",
    "print(pred_results.meanSquaredError)\n",
    "print(pred_results.r2)\n",
    "print(pred_results.rootMeanSquaredError)\n",
    "print(pred_results.r2adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d240c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "bm = model.bestModel\n",
    "model_input_train = data_model.transform(train)\n",
    "\n",
    "pred_results = bm.evaluate(model_input_train)\n",
    "print(pred_results.meanSquaredError)\n",
    "print(pred_results.r2)\n",
    "print(pred_results.rootMeanSquaredError)\n",
    "print(pred_results.r2adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec59416",
   "metadata": {},
   "outputs": [],
   "source": [
    "bm = model.bestModel\n",
    "model_input_train = data_model.transform(train)\n",
    "\n",
    "pred_results = bm.evaluate(model_input_train)\n",
    "print(pred_results.meanSquaredError)\n",
    "print(pred_results.r2)\n",
    "print(pred_results.rootMeanSquaredError)\n",
    "print(pred_results.r2adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22cc2a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_results.residuals.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa22ad3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
